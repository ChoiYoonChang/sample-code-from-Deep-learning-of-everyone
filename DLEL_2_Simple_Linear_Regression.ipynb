{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의의 코드를 구현한 것입니다.\n",
    "## Lab2 Simple Linear Regression\n",
    "### without placeholder\n",
    "placeholder를 사용하지 않았으므로 새로운 data에 대한 예측값을 구하기가 힘들다. 또한 아래의 코드에서는 gradient descent를 700번 실행하도록 iteration(코드 상에서 step 변수)의 횟수를 정했는데, 700번을 굳이 다하지 말고 cost의 값의 변화가 없으면 gradient descent를 멈추게 만들면 더 좋을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0410589 [ 1.24133587] [-0.43544704]\n",
      "20 0.0111618 [ 1.1227051] [-0.2789374]\n",
      "40 0.00421722 [ 1.07542384] [-0.1714562]\n",
      "60 0.00159337 [ 1.04636121] [-0.10538998]\n",
      "80 0.000602022 [ 1.02849722] [-0.06478079]\n",
      "100 0.00022746 [ 1.01751661] [-0.03981918]\n",
      "120 8.59404e-05 [ 1.01076698] [-0.02447597]\n",
      "140 3.24709e-05 [ 1.00661826] [-0.01504478]\n",
      "160 1.22684e-05 [ 1.00406802] [-0.0092477]\n",
      "180 4.63548e-06 [ 1.00250053] [-0.00568441]\n",
      "200 1.75148e-06 [ 1.00153708] [-0.0034941]\n",
      "220 6.61747e-07 [ 1.00094473] [-0.00214779]\n",
      "240 2.50043e-07 [ 1.00058079] [-0.00132018]\n",
      "260 9.44693e-08 [ 1.00035703] [-0.00081156]\n",
      "280 3.57052e-08 [ 1.00021946] [-0.00049891]\n",
      "300 1.34882e-08 [ 1.00013494] [-0.00030663]\n",
      "320 5.08923e-09 [ 1.00008285] [-0.00018851]\n",
      "340 1.92669e-09 [ 1.00005102] [-0.00011588]\n",
      "360 7.27165e-10 [ 1.00003135] [ -7.13076151e-05]\n",
      "380 2.76567e-10 [ 1.00001919] [ -4.37345043e-05]\n",
      "400 1.02618e-10 [ 1.0000118] [ -2.67869145e-05]\n",
      "420 4.0212e-11 [ 1.00000727] [ -1.65547808e-05]\n",
      "440 1.47852e-11 [ 1.00000453] [ -1.01810583e-05]\n",
      "460 5.36697e-12 [ 1.00000274] [ -6.18754711e-06]\n",
      "480 2.17071e-12 [ 1.00000167] [ -3.85104568e-06]\n",
      "500 9.48575e-13 [ 1.00000107] [ -2.42450756e-06]\n",
      "520 3.84877e-13 [ 1.0000006] [ -1.51851714e-06]\n",
      "540 1.71714e-13 [ 1.00000048] [ -9.90022272e-07]\n",
      "560 4.85538e-14 [ 1.00000036] [ -6.84051770e-07]\n",
      "580 3.78956e-14 [ 1.00000036] [ -6.24447182e-07]\n",
      "600 3.78956e-14 [ 1.00000036] [ -6.24447182e-07]\n",
      "620 3.78956e-14 [ 1.00000036] [ -6.24447182e-07]\n",
      "640 3.78956e-14 [ 1.00000036] [ -6.24447182e-07]\n",
      "660 3.78956e-14 [ 1.00000036] [ -6.24447182e-07]\n",
      "680 3.78956e-14 [ 1.00000036] [ -6.24447182e-07]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### without placeholder\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 1 and b 0, but Tensorflow will\n",
    "# figure that out for us\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# Our hypothesis\n",
    "hypothesis = W * x_data + b\n",
    "\n",
    "# Simplified cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "# Minimize\n",
    "alpha = tf.Variable(.1) # Learning rate, alpha\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Before starting, initizlie the variables. We will 'run' this first\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line\n",
    "for step in range(700):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with placeholder\n",
    "placeholder를 사용므로 새로운 data에 대한 예측값을 구할 수가 있다. 위의 경우와 마찬가지로  gradient descent를 700번 실행하도록 iteration(코드 상에서 step 변수)의 횟수를 정했는데, 700번을 굳이 다하지 말고 cost의 값의 변화가 없으면 gradient descent를 멈추게 만들면 더 좋을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.118072 [ 0.68454623] [ 0.85835183]\n",
      "20 0.0378315 [ 0.77409673] [ 0.51353121]\n",
      "40 0.0142937 [ 0.86114258] [ 0.31565541]\n",
      "60 0.00540056 [ 0.91464764] [ 0.19402589]\n",
      "80 0.00204048 [ 0.94753599] [ 0.11926316]\n",
      "100 0.000770948 [ 0.9677515] [ 0.07330824]\n",
      "120 0.000291285 [ 0.98017764] [ 0.04506085]\n",
      "140 0.000110054 [ 0.98781568] [ 0.02769778]\n",
      "160 4.15812e-05 [ 0.99251062] [ 0.01702517]\n",
      "180 1.57112e-05 [ 0.99539649] [ 0.01046498]\n",
      "200 5.93601e-06 [ 0.99717027] [ 0.00643255]\n",
      "220 2.24265e-06 [ 0.99826062] [ 0.00395393]\n",
      "240 8.47471e-07 [ 0.99893081] [ 0.00243045]\n",
      "260 3.20135e-07 [ 0.99934286] [ 0.00149393]\n",
      "280 1.20973e-07 [ 0.99959606] [ 0.00091832]\n",
      "300 4.57017e-08 [ 0.99975163] [ 0.0005645]\n",
      "320 1.72805e-08 [ 0.99984735] [ 0.00034701]\n",
      "340 6.51596e-09 [ 0.99990618] [ 0.00021328]\n",
      "360 2.46028e-09 [ 0.99994236] [ 0.00013108]\n",
      "380 9.33279e-10 [ 0.99996459] [  8.05847740e-05]\n",
      "400 3.49341e-10 [ 0.99997818] [  4.95108943e-05]\n",
      "420 1.3298e-10 [ 0.99998659] [  3.04294517e-05]\n",
      "440 5.10264e-11 [ 0.99999177] [  1.86992620e-05]\n",
      "460 1.93457e-11 [ 0.99999493] [  1.14910736e-05]\n",
      "480 6.99648e-12 [ 0.9999969] [  7.08827747e-06]\n",
      "500 2.89901e-12 [ 0.99999803] [  4.34646381e-06]\n",
      "520 1.04687e-12 [ 0.99999887] [  2.68548138e-06]\n",
      "540 3.97904e-13 [ 0.99999928] [  1.65233428e-06]\n",
      "560 1.56319e-13 [ 0.99999958] [  1.04039384e-06]\n",
      "580 6.15804e-14 [ 0.99999964] [  7.22502648e-07]\n",
      "600 9.4739e-14 [ 0.9999997] [  5.71504302e-07]\n",
      "620 3.78956e-14 [ 0.99999988] [  3.64874808e-07]\n",
      "640 4.73695e-15 [ 0.99999994] [  1.66192592e-07]\n",
      "660 0.0 [ 1.] [  5.49305597e-08]\n",
      "680 0.0 [ 1.] [  5.49305597e-08]\n",
      "[ 5.]\n",
      "[ 2.5]\n"
     ]
    }
   ],
   "source": [
    "### with placeholder\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis\n",
    "hypothesis = W * X + b\n",
    "\n",
    "# Simplified cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "alpha = tf.Variable(.1) # Learning rate, alpha\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Before starting, initizlie the variables. We will 'run' this first\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line\n",
    "for step in range(700):\n",
    "    sess.run(train, feed_dict = {X : x_data, Y : y_data})\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost, feed_dict = {X : x_data, Y : y_data}), sess.run(W), sess.run(b))\n",
    "\n",
    "# predict other x data\n",
    "print(sess.run(hypothesis, feed_dict = {X : 5}))\n",
    "print(sess.run(hypothesis, feed_dict = {X : 2.5}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
